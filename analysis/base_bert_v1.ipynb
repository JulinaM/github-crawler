{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.cloudant_utils import cloudant_db as db\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from prepare_sequence import prepareSequenceForBERT\n",
    "import random\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 512\n",
    "EPOCHS = 16\n",
    "RANDOM_SEED = 42\n",
    "model_name = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read the input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repos = [r for r in db.get_query_result({\"type\": \"release\"}, [\"_id\", \"releases\"], limit=10000, raw_result=True)[\"docs\"]]\n",
    "# values = [r for release in repos for r in release[\"releases\"]]\n",
    "# df = pd.DataFrame(values)\n",
    "# df['contributors'] = df['contributors'].apply(lambda x:\n",
    "#                                               [i for i in x if i is not None] if isinstance(x, list)\n",
    "#                                               else [])\n",
    "# df = df[~df['readme'].isnull()]\n",
    "# new_df = df.groupby(\"repo\").agg({\"readme\": list,\n",
    "#                                  \"total_stars\": list,\n",
    "# #                                  \"forks\": sum,\n",
    "# #                                  \"downloads\": sum,\n",
    "# #                                  \"contributors\": sum\n",
    "# #                                  \"releases\": count\n",
    "#                                 })\n",
    "# new_df = new_df[new_df['readme'].map(len) > 1]\n",
    "# # new_df = new_df[:10]\n",
    "# new_df['k'] = new_df['total_stars'].map(lambda x: random.randint(2, len(x)))\n",
    "# new_df['readme1'] = new_df.apply(lambda x: x.readme[:x.k], axis=1)\n",
    "# new_df['target_val'] = new_df.apply(lambda x: x.total_stars[x.k-1], axis=1)\n",
    "# new_df['sequence']= new_df['readme1'].apply(prepareSequenceForBERT)\n",
    "# new_df['target'] = new_df.apply(lambda x: 1 if x.target_val> 600 else 0, axis=1)\n",
    "# new_df.to_csv('csv/inputForBERT_' + current_time + '.csv')\n",
    "# new_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = pd.read_csv('csv/inputForBERT_2022_09_21-09_14PM.csv') \n",
    "new_df = pd.read_csv('csv/inputForBERT_2022_12_14-06_21PM.csv') \n",
    "new_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ReadmeDataSet(Dataset):\n",
    "   def __init__(self, _df, tokenizer, max_len):\n",
    "      self._df = _df\n",
    "      self.tokenizer = tokenizer\n",
    "      self.max_len = max_len\n",
    "\n",
    "   def __len__(self):\n",
    "      return len(self._df)\n",
    "\n",
    "   def __getitem__(self, item):\n",
    "      _sequence = self._df.iloc[item]['sequence']\n",
    "      target = self._df.iloc[item]['target']\n",
    "      encoding = self.tokenizer.encode_plus(_sequence,\n",
    "                                     None,\n",
    "                                     max_length = self.max_len,\n",
    "                                     truncation=True,\n",
    "                                     add_special_tokens=True,\n",
    "                                     padding='max_length',\n",
    "                                     return_token_type_ids=True)\n",
    "\n",
    "      return {\n",
    "      'sequence': _sequence,\n",
    "      'input_ids': torch.tensor(encoding.input_ids, dtype=torch.long),\n",
    "      'attention_mask':  torch.tensor(encoding.attention_mask, dtype=torch.long),\n",
    "      'token_type_ids': torch.tensor(encoding.token_type_ids, dtype=torch.long),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_data_loader(_df, tokenizer, max_len, batch_size):\n",
    "   ds = ReadmeDataSet(_df = _df, tokenizer=tokenizer, max_len=max_len)\n",
    "   return DataLoader(ds, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.1 Create the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForRepoClassification(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(BertForRepoClassification, self).__init__()\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.drop_out = nn.Dropout(0.5)\n",
    "#         self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.classifier = nn.Linear(768, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask,token_type_ids):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "#         last_hidden_state = last_hidden_state[:, 0]\n",
    "#         print(outputs)\n",
    "#         print(pooled_output.shape)\n",
    "        pooled_output = self.drop_out(outputs.pooler_output)\n",
    "        output = self.classifier(pooled_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Import BERT Tokenizer and BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "# bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "#                                                       num_labels=2,\n",
    "#                                                       output_attentions= False,\n",
    "#                                                       output_hidden_states= False)\n",
    "bert_model = BertForRepoClassification(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df_train, df_test = train_test_split(new_df, test_size=0.4, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data_loader = create_data_loader(df_train, bert_tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, bert_tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, bert_tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "new_df.shape, df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TEST the tokenizer and data loader\n",
    "# sequence = prepareSequenceForBERT(new_df.iloc[1]['readme'][:7])\n",
    "# label = new_df.iloc[1]['total_stars'][6]\n",
    "# tokens = bert_tokenizer.encode_plus(\n",
    "#             sequence,\n",
    "#             None,\n",
    "#             max_length= 512,\n",
    "#             truncation=True,\n",
    "#             add_special_tokens=True,\n",
    "# #             pad_to_max_length=True,\n",
    "#             padding = True,\n",
    "#             return_token_type_ids=True\n",
    "#         )\n",
    "# # print(f' Sentence: {sequence}')\n",
    "# # print(f' Tokens: {tokens}')\n",
    "# # print(f' Tokens.token_type_ids: {tokens.token_type_ids}')\n",
    "# # print(f' Tokens.input_ids: {len(tokens.input_ids)}')\n",
    "# output = {\n",
    "#       'input_ids': torch.tensor(tokens.input_ids, dtype=torch.long),\n",
    "#       'attention_mask':  torch.tensor(tokens.attention_mask, dtype=torch.long),\n",
    "#       'token_type_ids': torch.tensor(tokens.token_type_ids, dtype=torch.long),\n",
    "#       'targets': torch.tensor(label, dtype=torch.long)\n",
    "#     }\n",
    "# print(output['input_ids'].shape)\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# outputs = model(input_ids=output['input_ids'],attention_mask=output['attention_mask'])\n",
    "# print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# model = model.to(device)\n",
    "# for d in train_data_loader:\n",
    "#     input_ids = d[\"input_ids\"].to(device)\n",
    "#     attention_mask = d[\"attention_mask\"].to(device)\n",
    "#     token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "#     targets = d[\"targets\"].to(device)\n",
    "#     outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "#     print(outputs.last_hidden_state.shape)\n",
    "#     print(input_ids.shape)\n",
    "#     print(outputs.keys())\n",
    "#     print(d['sequence'])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(bert_model.parameters(), lr=2e-5, correct_bias=False, no_deprecation_warning=True)\n",
    "bert_model = bert_model.to(device)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch( model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "#     _, preds = torch.max(outputs.logits, dim=1)\n",
    "#     loss = loss_fn(outputs.logits, targets)\n",
    "    \n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)   \n",
    "\n",
    "#     all_linear1_params = torch.cat([x.view(-1) for x in model.parameters()])\n",
    "#     all_linear2_params = torch.cat([x.view(-1) for x in model.parameters()])\n",
    "#     l1_regularization = 0.3 * torch.norm(all_linear1_params, 1)\n",
    "#     l2_regularization = 0.01 * torch.norm(all_linear2_params, 2)\n",
    "\n",
    "#     loss = loss + l1_regularization + l2_regularization\n",
    "\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "#       _, preds = torch.max(outputs.logits, dim=1)\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "#       loss = loss_fn(outputs.logits, targets)\n",
    "      loss = loss_fn(outputs, targets)\n",
    "\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.tensor([1, 0, 1])\n",
    "# b = torch.tensor([0, 1, 0])\n",
    "# r = torch.sum(a == b)\n",
    "# r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "\n",
    "  sequences = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "\n",
    "      texts = d[\"sequence\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "#       _, preds = torch.max(outputs.logits, dim=1)\n",
    "#       probs = F.softmax(outputs.logits, dim=1)\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "      probs = F.softmax(outputs, dim=1)\n",
    "#       print(preds, probs, targets)\n",
    "      sequences.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(probs)\n",
    "      real_values.extend(targets)\n",
    "\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  return outputs, sequences, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(40*\"*\", 'Training')\n",
    "current_time = datetime.now().strftime(\"%Y_%m_%d-%I_%M%p\")\n",
    "print(current_time)\n",
    "history = defaultdict(list)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "best_accuracy = 0\n",
    "# progress_bar = tqdm(range(total_steps))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(bert_model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train))\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(bert_model, val_data_loader, loss_fn, device, len(df_val) )\n",
    "\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "      curr_time = datetime.now().strftime(\"%Y_%m_%d-%I_%M%p\")\n",
    "      torch.save(bert_model.state_dict(), 'checkpoint/best_model_state' + current_time+'.bin')\n",
    "      best_accuracy = val_acc\n",
    "#     progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history['train_acc'], label='train accuracy')\n",
    "# plt.plot(history['val_acc'], label='validation accuracy')\n",
    "\n",
    "# plt.title('Training history')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend()\n",
    "# plt.ylim([0, 1]);\n",
    "# plt.savefig('images/training_history '+current_time+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs_v, y_sequences_v, y_pred_v, y_pred_probs_v, y_test_v = get_predictions(bert_model, val_data_loader)\n",
    "# generate_auc(y_test_v.numpy(), y_pred_probs_v[:, 1].numpy(), current_time)\n",
    "# display_prediction_df(y_sequences_v, y_test_v, y_pred_v, y_pred_probs_v, current_time, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_prediction_df(y_sequences, y_test, y_pred, y_pred_probs, fileName=None ):\n",
    "    y_pred_probs_pd = [y.numpy() for y in y_pred_probs]\n",
    "    someListOfLists = list(zip(y_sequences, y_test.numpy(), y_pred.numpy(), y_pred_probs[:, 1:].numpy().squeeze(), y_pred_probs_pd ))\n",
    "    npa = np.asarray(someListOfLists)\n",
    "    dff = pd.DataFrame(someListOfLists, columns = ['readme', 'Real', 'Predicted', 'Pred-prob', 'All Pred-probs' ])\n",
    "    print(dff)\n",
    "    if fileName:\n",
    "        dff.to_csv('out/pred_result_'+ fileName+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'out/'+ current_time\n",
    "print('creating folder path '+  path)\n",
    "import os\n",
    "try: \n",
    "    os.mkdir(path) \n",
    "except OSError as error: \n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def generate_auc(label, pred, c_pred=None, both=False, fileName=None):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "\n",
    "#     pred = y_pred_probs[:, 1].numpy()\n",
    "#     label = y_test.numpy()\n",
    "    fpr, tpr, thresh = metrics.roc_curve(label, pred)\n",
    "    auc = metrics.roc_auc_score(label, pred)\n",
    "    plt.plot(fpr, tpr, label=f'BERT, auc = {str(round(auc,3))}')\n",
    "\n",
    "    if both:\n",
    "        pred = c_pred\n",
    "        fpr, tpr, thresh = metrics.roc_curve(label, c_pred)\n",
    "        auc = metrics.roc_auc_score(label, c_pred)\n",
    "        plt.plot(fpr, tpr, label=f'Calibrated BERT, auc = {str(round(auc,3))}')\n",
    "\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.title(\"AUC-ROC\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    if fileName:\n",
    "        plt.savefig(fileName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    " \n",
    "class IdentityEstimator(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "    An identity estimator used for calibrating probability data\n",
    "    '''\n",
    " \n",
    "    def __init__(self):\n",
    "        self.classes_= [0, 1]\n",
    "        # pass\n",
    "       \n",
    " \n",
    "    def __getstate__(self):\n",
    "        # Copy the object's state from self.__dict__ which contains\n",
    "        # all our instance attributes. Always use the dict.copy()\n",
    "        # method to avoid modifying the original state.\n",
    "        state = self.__dict__.copy()\n",
    " \n",
    "        return state\n",
    " \n",
    "    def __setstate__(self, state):\n",
    "        # Restore instance attributes\n",
    "        self.__dict__.update(state)\n",
    "        \n",
    " \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        return self\n",
    " \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        assert X.shape[1] == 1\n",
    "        probs = np.concatenate((1 - X, X), axis=1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "def plot_calibration_curve(name, fig_index, probs, y_test, fileName=None):\n",
    "    \"\"\"Plot calibration curve for est w/o and with calibration. \"\"\"\n",
    "\n",
    "    fig = plt.figure(fig_index, figsize=(10, 10))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "    \n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    \n",
    "    frac_of_pos, mean_pred_value = calibration_curve(y_test, probs, n_bins=20 , normalize=True)\n",
    "\n",
    "    ax1.plot(mean_pred_value, frac_of_pos, \"s-\", label=f'{name}')\n",
    "    ax1.set_ylabel(\"Fraction of positives\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.set_title(f'Calibration plot ({name})')\n",
    "    \n",
    "    ax2.hist(probs, range=(0, 1), bins=20, label=name, histtype=\"step\", lw=2)\n",
    "    ax2.set_xlabel(\"Mean predicted value\")\n",
    "    ax2.set_ylabel(\"Count\")\n",
    "    if date:\n",
    "        plt.savefig(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outputss, y_sequences, y_pred, y_pred_probs, y_test = get_predictions(bert_model, train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_auc(y_test.numpy(), y_pred_probs[:, 1].numpy(), fileName='out/'+current_time+'/LOC_train.png' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, _ = eval_model(bert_model, test_data_loader, loss_fn, device, len(df_test))\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model = BertForRepoClassification(2)\n",
    "# bert_model.load_state_dict(torch.load('checkpoint/best_model_state2022_11_30-11_28AM.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result without Calibrated Classifier Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputss_t, y_sequences_t, y_pred_t, y_pred_probs_t, y_test_t = get_predictions(bert_model, val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_prediction_df(y_sequences, y_test_t, y_pred_t, y_pred_probs_t, 'test_'+ current_time )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_auc(y_test_t.numpy(), y_pred_probs_t[:, 1].numpy(), fileName='out/'+current_time+'/LOC_val.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calibration_curve(\"BERT\", 1, y_pred_probs_t[:, 1].numpy(), y_test_t.numpy(), fileName='out/'+current_timee+'/wo_calib.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result after Calibrated Classifier Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_est = IdentityEstimator()\n",
    "# probs = id_est.predict_proba(y_pred_probs[:, 1:])\n",
    "# print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "calibrated_clf = CalibratedClassifierCV(base_estimator=id_est, cv=\"prefit\", method=\"isotonic\")\n",
    "calibrated_clf.fit(y_pred_probs[:, 1:], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_c = calibrated_clf.predict_proba(y_pred_probs_t[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calibration_curve(\"Caliberated BERT\", 1, prob_c[:, 1], y_test_t.numpy(), fileName='out/'+current_time+'/w_calib.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_auc(y_test_t.numpy(),  prob_c[:, 1], fileName='out/'+current_time+'/LOC_val_w_calib.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
